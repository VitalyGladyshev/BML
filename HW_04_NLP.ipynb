{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_04_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+j0cAQz4njUCwVVuMKltS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VitalyGladyshev/BML/blob/master/HW_04_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6tnYGTo1NAg"
      },
      "source": [
        "# ДЗ 04 NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBAKy9m03zRO"
      },
      "source": [
        "В качестве заготовки для задания прогоним часть 2ого домашнего задания. Нам необходимо получить разреженные матрицы, используя CountVectorizer, TfidfVectorizer для 'tweet_stemmed' и 'tweet_lemmatized' столбцов (4 матрицы)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXU96IsF0nnL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiasjrMi1Y1v",
        "outputId": "7c36f5a2-82f4-4c05-a76a-8f012c72e49a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSRSwudx44cu"
      },
      "source": [
        "path_nlp = \"/content/gdrive/My Drive/Colab Notebooks/NLP/\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja7jZAyg1Y_i"
      },
      "source": [
        "train_cor = pd.read_pickle(path_nlp + \"train_cor.pkl\")\n",
        "test_cor = pd.read_pickle(path_nlp + \"test_cor.pkl\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hx3PDNL1kMc",
        "outputId": "2d2793b3-b6dc-4012-e255-3c486cf4c021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "test_cor.tail(2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>clean_tweet</th>\n",
              "      <th>tweet_token</th>\n",
              "      <th>tweet_token_filtered</th>\n",
              "      <th>tweet_stemmed</th>\n",
              "      <th>tweet_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49157</th>\n",
              "      <td>49158</td>\n",
              "      <td>happy, at work conference: right mindset leads...</td>\n",
              "      <td>happy at work conference right mindset leads t...</td>\n",
              "      <td>[happy, at, work, conference, right, mindset, ...</td>\n",
              "      <td>[happy, work, conference, right, mindset, lead...</td>\n",
              "      <td>{right, happi, organ, lead, cultureofdevelop, ...</td>\n",
              "      <td>{mindset, lead, cultureofdevelopment, conferen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49158</th>\n",
              "      <td>49159</td>\n",
              "      <td>my   song \"so glad\" free download!  #shoegaze ...</td>\n",
              "      <td>my song so glad free download shoegaze newmusi...</td>\n",
              "      <td>[my, song, so, glad, free, download, shoegaze,...</td>\n",
              "      <td>[song, glad, free, download, shoegaze, newmusi...</td>\n",
              "      <td>{glad, shoegaz, download, newsong, song, newmu...</td>\n",
              "      <td>{glad, download, newsong, song, newmusic, free...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                   tweet_lemmatized\n",
              "49157  49158  ...  {mindset, lead, cultureofdevelopment, conferen...\n",
              "49158  49159  ...  {glad, download, newsong, song, newmusic, free...\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw_xHyKr1kQ_"
      },
      "source": [
        "test_cor['label'] = -1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it4KIwsN1nxs",
        "outputId": "d17693fc-f78b-47b5-d789-db6f49a2140e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "data = train_cor.copy()\n",
        "data = data.append(test_cor, ignore_index=True)\n",
        "data.tail(2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>clean_tweet</th>\n",
              "      <th>tweet_token</th>\n",
              "      <th>tweet_token_filtered</th>\n",
              "      <th>tweet_stemmed</th>\n",
              "      <th>tweet_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49157</th>\n",
              "      <td>49158</td>\n",
              "      <td>-1</td>\n",
              "      <td>happy, at work conference: right mindset leads...</td>\n",
              "      <td>happy at work conference right mindset leads t...</td>\n",
              "      <td>[happy, at, work, conference, right, mindset, ...</td>\n",
              "      <td>[happy, work, conference, right, mindset, lead...</td>\n",
              "      <td>{right, happi, organ, lead, cultureofdevelop, ...</td>\n",
              "      <td>{mindset, lead, cultureofdevelopment, conferen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49158</th>\n",
              "      <td>49159</td>\n",
              "      <td>-1</td>\n",
              "      <td>my   song \"so glad\" free download!  #shoegaze ...</td>\n",
              "      <td>my song so glad free download shoegaze newmusi...</td>\n",
              "      <td>[my, song, so, glad, free, download, shoegaze,...</td>\n",
              "      <td>[song, glad, free, download, shoegaze, newmusi...</td>\n",
              "      <td>{glad, shoegaz, download, newsong, song, newmu...</td>\n",
              "      <td>{glad, download, newsong, song, newmusic, free...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                   tweet_lemmatized\n",
              "49157  49158  ...  {mindset, lead, cultureofdevelopment, conferen...\n",
              "49158  49159  ...  {glad, download, newsong, song, newmusic, free...\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDuS4Edc1n1x"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KYPRHvX1y-E",
        "outputId": "089167d5-205e-4d4d-f2f2-50944795e5ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "count_vectorizer_stemmed = CountVectorizer(stop_words='english', max_df=0.9, max_features=1000)\n",
        "count_vectorizer_lemmatized = CountVectorizer(stop_words='english', max_df=0.9, max_features=1000)\n",
        "\n",
        "bag_of_words_stemmed = count_vectorizer_stemmed.fit_transform(list(map(lambda x: \" \".join(x), data['tweet_stemmed'].values)))\n",
        "bag_of_words_lemmatized = count_vectorizer_lemmatized.fit_transform(list(map(lambda x: \" \".join(x), data['tweet_lemmatized'].values)))\n",
        "\n",
        "df_count_vect_stemmed = pd.DataFrame(bag_of_words_stemmed.toarray(), columns=count_vectorizer_stemmed.get_feature_names())\n",
        "df_count_vect_lemmatized = pd.DataFrame(bag_of_words_lemmatized.toarray(), columns=count_vectorizer_lemmatized.get_feature_names())\n",
        "df_count_vect_lemmatized.head(2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>able</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>accept</th>\n",
              "      <th>account</th>\n",
              "      <th>act</th>\n",
              "      <th>action</th>\n",
              "      <th>actor</th>\n",
              "      <th>actually</th>\n",
              "      <th>adapt</th>\n",
              "      <th>add</th>\n",
              "      <th>adventure</th>\n",
              "      <th>advice</th>\n",
              "      <th>affirmation</th>\n",
              "      <th>affirmations</th>\n",
              "      <th>afternoon</th>\n",
              "      <th>age</th>\n",
              "      <th>ago</th>\n",
              "      <th>agree</th>\n",
              "      <th>ahead</th>\n",
              "      <th>aist</th>\n",
              "      <th>album</th>\n",
              "      <th>alive</th>\n",
              "      <th>allahsoil</th>\n",
              "      <th>alligator</th>\n",
              "      <th>allow</th>\n",
              "      <th>altwaystoheal</th>\n",
              "      <th>amaze</th>\n",
              "      <th>america</th>\n",
              "      <th>american</th>\n",
              "      <th>americans</th>\n",
              "      <th>amp</th>\n",
              "      <th>anger</th>\n",
              "      <th>angry</th>\n",
              "      <th>animals</th>\n",
              "      <th>anniversary</th>\n",
              "      <th>announce</th>\n",
              "      <th>answer</th>\n",
              "      <th>anxiety</th>\n",
              "      <th>anymore</th>\n",
              "      <th>app</th>\n",
              "      <th>...</th>\n",
              "      <th>white</th>\n",
              "      <th>wife</th>\n",
              "      <th>wild</th>\n",
              "      <th>win</th>\n",
              "      <th>wine</th>\n",
              "      <th>winner</th>\n",
              "      <th>wish</th>\n",
              "      <th>woh</th>\n",
              "      <th>woman</th>\n",
              "      <th>women</th>\n",
              "      <th>wonder</th>\n",
              "      <th>wonderful</th>\n",
              "      <th>word</th>\n",
              "      <th>work</th>\n",
              "      <th>workout</th>\n",
              "      <th>world</th>\n",
              "      <th>worry</th>\n",
              "      <th>worse</th>\n",
              "      <th>worst</th>\n",
              "      <th>wow</th>\n",
              "      <th>write</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wtf</th>\n",
              "      <th>xx</th>\n",
              "      <th>xxx</th>\n",
              "      <th>ya</th>\n",
              "      <th>yay</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yo</th>\n",
              "      <th>yoga</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>youtube</th>\n",
              "      <th>yr</th>\n",
              "      <th>yrs</th>\n",
              "      <th>yummy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   able  absolutely  accept  account  act  ...  young  youtube  yr  yrs  yummy\n",
              "0     0           0       0        0    0  ...      0        0   0    0      0\n",
              "1     0           0       0        0    0  ...      0        0   0    0      0\n",
              "\n",
              "[2 rows x 1000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx4JxaWR1zID",
        "outputId": "c3d6256c-3bdb-46e7-f7c7-ab9457e75c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "tfidf_vectorizer_stemmed = TfidfVectorizer(stop_words='english', max_df=0.9, max_features=1000)\n",
        "tfidf_vectorizer_lemmatized = TfidfVectorizer(stop_words='english', max_df=0.9, max_features=1000)\n",
        "\n",
        "tfidf_stemmed = tfidf_vectorizer_stemmed.fit_transform(list(map(lambda x: \" \".join(x), data['tweet_stemmed'].values)))\n",
        "tfidf_lemmatized = tfidf_vectorizer_lemmatized.fit_transform(list(map(lambda x: \" \".join(x), data['tweet_lemmatized'].values)))\n",
        "\n",
        "df_tfidf_vect_stemmed = pd.DataFrame(tfidf_stemmed.toarray(), columns=tfidf_vectorizer_stemmed.get_feature_names())\n",
        "df_tfidf_vect_lemmatized = pd.DataFrame(tfidf_lemmatized.toarray(), columns=tfidf_vectorizer_lemmatized.get_feature_names())\n",
        "df_tfidf_vect_lemmatized.head(2)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>able</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>accept</th>\n",
              "      <th>account</th>\n",
              "      <th>act</th>\n",
              "      <th>action</th>\n",
              "      <th>actor</th>\n",
              "      <th>actually</th>\n",
              "      <th>adapt</th>\n",
              "      <th>add</th>\n",
              "      <th>adventure</th>\n",
              "      <th>advice</th>\n",
              "      <th>affirmation</th>\n",
              "      <th>affirmations</th>\n",
              "      <th>afternoon</th>\n",
              "      <th>age</th>\n",
              "      <th>ago</th>\n",
              "      <th>agree</th>\n",
              "      <th>ahead</th>\n",
              "      <th>aist</th>\n",
              "      <th>album</th>\n",
              "      <th>alive</th>\n",
              "      <th>allahsoil</th>\n",
              "      <th>alligator</th>\n",
              "      <th>allow</th>\n",
              "      <th>altwaystoheal</th>\n",
              "      <th>amaze</th>\n",
              "      <th>america</th>\n",
              "      <th>american</th>\n",
              "      <th>americans</th>\n",
              "      <th>amp</th>\n",
              "      <th>anger</th>\n",
              "      <th>angry</th>\n",
              "      <th>animals</th>\n",
              "      <th>anniversary</th>\n",
              "      <th>announce</th>\n",
              "      <th>answer</th>\n",
              "      <th>anxiety</th>\n",
              "      <th>anymore</th>\n",
              "      <th>app</th>\n",
              "      <th>...</th>\n",
              "      <th>white</th>\n",
              "      <th>wife</th>\n",
              "      <th>wild</th>\n",
              "      <th>win</th>\n",
              "      <th>wine</th>\n",
              "      <th>winner</th>\n",
              "      <th>wish</th>\n",
              "      <th>woh</th>\n",
              "      <th>woman</th>\n",
              "      <th>women</th>\n",
              "      <th>wonder</th>\n",
              "      <th>wonderful</th>\n",
              "      <th>word</th>\n",
              "      <th>work</th>\n",
              "      <th>workout</th>\n",
              "      <th>world</th>\n",
              "      <th>worry</th>\n",
              "      <th>worse</th>\n",
              "      <th>worst</th>\n",
              "      <th>wow</th>\n",
              "      <th>write</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wtf</th>\n",
              "      <th>xx</th>\n",
              "      <th>xxx</th>\n",
              "      <th>ya</th>\n",
              "      <th>yay</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yo</th>\n",
              "      <th>yoga</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>youtube</th>\n",
              "      <th>yr</th>\n",
              "      <th>yrs</th>\n",
              "      <th>yummy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   able  absolutely  accept  account  act  ...  young  youtube   yr  yrs  yummy\n",
              "0   0.0         0.0     0.0      0.0  0.0  ...    0.0      0.0  0.0  0.0    0.0\n",
              "1   0.0         0.0     0.0      0.0  0.0  ...    0.0      0.0  0.0  0.0    0.0\n",
              "\n",
              "[2 rows x 1000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJK0uQy22qgz"
      },
      "source": [
        "## Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktPJhpF02t20"
      },
      "source": [
        "Построим модель LogisticRegression, используя Bag-of-Words признаки для столбца combine_df['tweet_stemmed']. \n",
        "- Поделим Bag-of-Words признаки на train, test (train заканчивается на 31962 строке combine_df)\n",
        "- Ответами является столбец train_df['label']\n",
        "- Рассчитаем predict_proba, приведем prediction в в бинарный вид: если предсказание >= 0.3 то 1, иначе 0, тип заменим на int\n",
        "- Рассчитаем f1_score \n",
        "\n",
        "Повторим аналогично для столбца combine_df['tweet_lemmatized']."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUqhEUKNoAC5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import scipy\n",
        "from typing import Set"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twnL97kfq6DX",
        "outputId": "99d7b9a8-ad37-4816-f830-2aa400ba82f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.loc[data['label'] != -1, 'label'].values"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FplZl-tPvhoN",
        "outputId": "a527672c-2c19-4a83-f6af-1d6999e0c90d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(bag_of_words_stemmed)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIHyufO9q96o"
      },
      "source": [
        "def log_reg_model_f1_score(X: scipy.sparse.csr.csr_matrix, y: np.array) -> Set:\n",
        "    train_x, valid_x, train_y, valid_y = train_test_split(X, \n",
        "                                                          y, \n",
        "                                                          stratify=y, \n",
        "                                                          random_state=43)\n",
        "\n",
        "    model = LogisticRegression()\n",
        "\n",
        "    model.fit(train_x, train_y)\n",
        "        \n",
        "    predictions = model.predict(valid_x)\n",
        "    predictions_prb = model.predict_proba(valid_x)\n",
        "\n",
        "    score = f1_score(valid_y, predictions)\n",
        "    print(f\"f1 score: {score:.4f}\")\n",
        "\n",
        "    pred = np.zeros(predictions_prb.shape[0])\n",
        "    pred[predictions_prb[:, 1] >= 0.3] = 1\n",
        "    pred = pred.astype(int)\n",
        "    score = f1_score(valid_y, pred)\n",
        "    print(f\"f1 score порог 0.3: {score:.4f}\")\n",
        "\n",
        "    return predictions, predictions_prb, pred"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96DjhrEnn87U",
        "outputId": "647090a0-675f-4fed-fc86-24cf0292dd66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pred_bw_st, pred_bw_st_prb, pred_bw_st_prb_03 = log_reg_model_f1_score(bag_of_words_stemmed[:31962], train_cor['label'].values)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1 score: 0.5365\n",
            "f1 score порог 0.3: 0.5787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKXBcRXr6XRT",
        "outputId": "cc93a174-e691-48b4-81ae-ee981fabee68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pred_bw_lm, pred_bw_lm_prb, pred_bw_lm_prb_03 = log_reg_model_f1_score(bag_of_words_lemmatized[:31962], train_cor['label'].values)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1 score: 0.5085\n",
            "f1 score порог 0.3: 0.5647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T569Ldt321q2"
      },
      "source": [
        "## Задание 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLc4rx_e21kP"
      },
      "source": [
        "Построим модель LogisticRegression, используя TF-IDF признаки для столбца combine_df['tweet_stemmed']. \n",
        "- Поделим TF-IDF признаки на train, test (train заканчивается на 31962 строке combine_df)\n",
        "- Ответами является столбец train_df['label']\n",
        "- Рассчитаем predict_proba, приведем prediction в в бинарный вид: если предсказание >= 0.3 то 1, иначе 0, тип заменим на int\n",
        "- Рассчитаем f1_score \n",
        "\n",
        "Повторим аналогично для столбца combine_df['tweet_lemmatized']."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s83dxd5V1zC1",
        "outputId": "ff5de880-de11-4575-d87b-35e9360fbae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pred_tf_st, pred_tf_st_prb, pred_tf_st_prb_03 = log_reg_model_f1_score(tfidf_stemmed[:31962], train_cor['label'].values)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1 score: 0.5168\n",
            "f1 score порог 0.3: 0.5774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5jewa8B7Fyy",
        "outputId": "c03fdbc1-848a-47cf-9c5f-7182a3544610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pred_tf_lm, pred_tf_lm_prb, pred_tf_lm_prb_03 = log_reg_model_f1_score(tfidf_lemmatized[:31962], train_cor['label'].values)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1 score: 0.4898\n",
            "f1 score порог 0.3: 0.5607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy_U95aa3F15"
      },
      "source": [
        "## Задание 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ltdv5TZ3Fxe"
      },
      "source": [
        "Выведите результаты f1-score всех моделей, сделайте вывод."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKEpPd8N3Kvf"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2y7tW9A-Dn0"
      },
      "source": [
        "def metrics_view(y_valid: np.ndarray, y_pred: np.ndarray):\n",
        "    print(f\"confusion_matrix: \\n{confusion_matrix(y_valid, y_pred)}\\n\")\n",
        "    print(f\"precision:\\t{precision_score(y_valid, y_pred):.4f}\")\n",
        "    print(f\"recall:\\t\\t{recall_score(y_valid, y_pred):.4f}\")\n",
        "    print(f\"roc auc:\\t{roc_auc_score(y_valid, y_pred):.4f}\")\n",
        "    print(f\"f1:\\t\\t{f1_score(y_valid, y_pred):.4f}\")"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpjzFCnc-EPT",
        "outputId": "11234d11-7257-466c-e6f5-1134df08a64e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "metrics_view(valid_y, pred_bw_st)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion_matrix: \n",
            "[[7173  257]\n",
            " [ 544   17]]\n",
            "\n",
            "precision:\t0.0620\n",
            "recall:\t\t0.0303\n",
            "roc auc:\t0.4979\n",
            "f1:\t\t0.0407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5laETFhL9rO",
        "outputId": "05b276cb-00de-4453-e38a-8a261a6d768d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "metrics_view(valid_y, pred_bw_st_prb_03)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion_matrix: \n",
            "[[7003  427]\n",
            " [ 533   28]]\n",
            "\n",
            "precision:\t0.0615\n",
            "recall:\t\t0.0499\n",
            "roc auc:\t0.4962\n",
            "f1:\t\t0.0551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-LhH71g-EK4",
        "outputId": "42b088f6-7656-4a1f-978b-436b5a5991f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "metrics_view(valid_y, pred_bw_lm_prb_03)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion_matrix: \n",
            "[[7045  385]\n",
            " [ 533   28]]\n",
            "\n",
            "precision:\t0.0678\n",
            "recall:\t\t0.0499\n",
            "roc auc:\t0.4990\n",
            "f1:\t\t0.0575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sJmpi04-DfN",
        "outputId": "cf19f403-2983-4515-ee41-1791a1845b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "metrics_view(valid_y, pred_tf_st_prb_03)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion_matrix: \n",
            "[[7053  377]\n",
            " [ 536   25]]\n",
            "\n",
            "precision:\t0.0622\n",
            "recall:\t\t0.0446\n",
            "roc auc:\t0.4969\n",
            "f1:\t\t0.0519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYv6f10hB6Nw",
        "outputId": "8b0e638e-60b6-432f-e296-74ec62df3cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "metrics_view(valid_y, pred_tf_lm_prb_03)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion_matrix: \n",
            "[[7084  346]\n",
            " [ 537   24]]\n",
            "\n",
            "precision:\t0.0649\n",
            "recall:\t\t0.0428\n",
            "roc auc:\t0.4981\n",
            "f1:\t\t0.0516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvyb6eJ8CDpQ"
      },
      "source": [
        "## Выводы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOOmDaAmCKkh"
      },
      "source": [
        "Все построенные модели характеризуются низкими значниями precision и recall - дают много ложно положительных и ложно отрицательных значений и соответственно низкое значение целевой метрики f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx6a9E8w3RJ0"
      },
      "source": [
        "## Задание 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_s7h4ka3REC"
      },
      "source": [
        "Теперь перейдем к визуализации. Посмотрим, какие слова являются наиболее популярные в датасете с помощью облака слов (WordCloud).\n",
        "Облако слов - это визуализация, в которой наиболее частые слова большого размера, а менее частые слова меньшего размера.\n",
        "- объединим слова в одну строку\n",
        "- создадим словарь частот слов с помощью collections.Counter\n",
        "- нарисуем облако слов с частотами слов с помощью WordCloud.generate_from_frequencies()\n",
        "- используем nltk.corpus.stopwords как параметр stopwords, чтобы убрать \"мусорные\" частотные слова"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz147yfe3ajf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMc5QXfD3bqw"
      },
      "source": [
        "## Задание 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77_X7lgy3bmt"
      },
      "source": [
        "Теперь отобразим облако слов для отзывов, не содержащих токсичных комментариев (combine_df['label'] == 0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0_e9EYE3kzP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuwFAtID3liZ"
      },
      "source": [
        "## Задание 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lT7aWLY3ldh"
      },
      "source": [
        "Теперь отобразим облако слов для отзывов, содержащих токсичные комментарии (combine_df['label'] == 1). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBsAm5N53yHc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}