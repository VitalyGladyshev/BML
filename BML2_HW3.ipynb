{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ 3 Гладышев В.В.\n",
    "\n",
    "## Определение ССЗ\n",
    "\n",
    "### Загрузка и предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0   0  18393       2     168    62.0    110     80            1     1      0   \n",
       "1   1  20228       1     156    85.0    140     90            3     1      0   \n",
       "2   2  18857       1     165    64.0    130     70            3     1      0   \n",
       "\n",
       "   alco  active  cardio  \n",
       "0     0       1       0  \n",
       "1     0       1       1  \n",
       "2     0       0       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_case2.csv', ';')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разделим данные на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('cardio', 1), \n",
    "                                                    df['cardio'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "    \n",
    "class OHEEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.columns = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.columns = [col for col in pd.get_dummies(X, prefix=self.key).columns]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.get_dummies(X, prefix=self.key)\n",
    "        test_columns = [col for col in X.columns]\n",
    "        for col_ in test_columns:\n",
    "            if col_ not in self.columns:\n",
    "                X[col_] = 0\n",
    "        return X[self.columns]\n",
    "\n",
    "\n",
    "continuos_cols = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']\n",
    "cat_cols = ['gender', 'cholesterol']\n",
    "base_cols = ['gluc', 'smoke', 'alco', 'active']\n",
    "\n",
    "continuos_transformers = []\n",
    "cat_transformers = []\n",
    "base_transformers = []\n",
    "\n",
    "for cont_col in continuos_cols:\n",
    "    transfomer =  Pipeline([\n",
    "                ('selector', NumberSelector(key=cont_col)),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "    continuos_transformers.append((cont_col, transfomer))\n",
    "    \n",
    "for cat_col in cat_cols:\n",
    "    cat_transformer = Pipeline([\n",
    "                ('selector', ColumnSelector(key=cat_col)),\n",
    "                ('ohe', OHEEncoder(key=cat_col))\n",
    "            ])\n",
    "    cat_transformers.append((cat_col, cat_transformer))\n",
    "    \n",
    "for base_col in base_cols:\n",
    "    base_transformer = Pipeline([\n",
    "                ('selector', NumberSelector(key=base_col))\n",
    "            ])\n",
    "    base_transformers.append((base_col, base_transformer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.73391771,  0.6873301 ,  0.74843904, ...,  1.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-1.67343538,  0.07758923, -0.29640123, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.13738132,  1.17512278, -0.15708919, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 1.17775864,  1.17512278, -0.15708919, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-0.47190715, -1.38578883,  0.74843904, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.38174619,  0.56538192, -0.08743318, ...,  0.        ,\n",
       "         0.        ,  1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "feats = FeatureUnion(continuos_transformers+cat_transformers+base_transformers)\n",
    "feature_processing = Pipeline([('feats', feats)])\n",
    "\n",
    "feature_processing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Таблица для сохранения результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_clissifiers = ['LogisticRegression', \n",
    "                 'KNeighborsClassifier', \n",
    "                 'RandomForestClassifier', \n",
    "                 'GradientBoostingClassifier', \n",
    "                 'XGBClassifier']\n",
    "\n",
    "cl_met = ['Best Threshold', 'F-Score', 'Precision', \n",
    "          'Recall', 'roc_auc_s', 'log_loss_s', 'TPR', 'FPR', 'TNR']\n",
    "\n",
    "res_tab = pd.DataFrame(columns=cl_met)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score is 0.7867401104915408+-0.00852135511666111\n"
     ]
    }
   ],
   "source": [
    "classifier = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', LogisticRegression(random_state = 42)),\n",
    "])\n",
    "\n",
    "\n",
    "#запустим кросс-валидацию\n",
    "cv_scores = cross_val_score(classifier, X_train, y_train, cv=16, scoring='roc_auc')\n",
    "cv_score = np.mean(cv_scores)\n",
    "cv_score_std = np.std(cv_scores)\n",
    "print('CV score is {}+-{}'.format(cv_score, cv_score_std))\n",
    "\n",
    "#обучим пайплайн на всем тренировочном датасете\n",
    "classifier.fit(X_train, y_train)\n",
    "y_score = classifier.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.386937, F-Score=0.730, Precision=0.647, Recall=0.838\n"
     ]
    }
   ],
   "source": [
    "b=1\n",
    "precision, recall, thresholds = precision_recall_curve(y_test.values, y_score)\n",
    "fscore = (1+b**2)*(precision * recall) / (b**2*precision + recall)\n",
    "# locate the index of the largest f score\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n",
    "                                                                        fscore[ix],\n",
    "                                                                        precision[ix],\n",
    "                                                                        recall[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc score: 0.7840347790421852\n",
      "log loss score: 0.5779604008230663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "r_auc = roc_auc_score(y_true=y_test, y_score=classifier.predict_proba(X_test)[:,1])\n",
    "l_los = log_loss(y_true=y_test, y_pred=classifier.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(\"roc auc score: {}\".format(r_auc))\n",
    "print(\"log loss score: {}\".format(l_los))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.837442396313364, 0.44886621315192743, 0.5511337868480726)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_score>thresholds[ix])\n",
    "\n",
    "TN = cnf_matrix[0][0]\n",
    "FN = cnf_matrix[1][0]\n",
    "TP = cnf_matrix[1][1]\n",
    "FP = cnf_matrix[0][1]\n",
    "\n",
    "TPR = TP/(TP+FN)\n",
    "FPR = FP/(FP+TN)\n",
    "TNR = TN/(FP+TN)\n",
    "TPR, FPR, TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tab.loc['LogisticRegression', :] = [thresholds[ix], fscore[ix], precision[ix], recall[ix], r_auc, l_los, TPR, FPR, TNR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Threshold</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>roc_auc_s</th>\n",
       "      <th>log_loss_s</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.386937</td>\n",
       "      <td>0.730323</td>\n",
       "      <td>0.647431</td>\n",
       "      <td>0.837558</td>\n",
       "      <td>0.784035</td>\n",
       "      <td>0.57796</td>\n",
       "      <td>0.837442</td>\n",
       "      <td>0.448866</td>\n",
       "      <td>0.551134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Best Threshold   F-Score Precision    Recall roc_auc_s  \\\n",
       "LogisticRegression       0.386937  0.730323  0.647431  0.837558  0.784035   \n",
       "\n",
       "                   log_loss_s       TPR       FPR       TNR  \n",
       "LogisticRegression    0.57796  0.837442  0.448866  0.551134  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score is 0.6919218138274713+-0.007083802929940452\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "])\n",
    "\n",
    "#запустим кросс-валидацию\n",
    "cv_scores = cross_val_score(knn, X_train, y_train, cv=16, scoring='roc_auc')\n",
    "cv_score = np.mean(cv_scores)\n",
    "cv_score_std = np.std(cv_scores)\n",
    "print('CV score is {}+-{}'.format(cv_score, cv_score_std))\n",
    "\n",
    "#обучим пайплайн на всем тренировочном датасете\n",
    "knn.fit(X_train, y_train)\n",
    "y_score = knn.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.400000, F-Score=0.686, Precision=0.588, Recall=0.822\n",
      "roc auc score: 0.6940883204280176\n",
      "log loss score: 2.0495594233752303\n"
     ]
    }
   ],
   "source": [
    "b=1\n",
    "precision, recall, thresholds = precision_recall_curve(y_test.values, y_score)\n",
    "fscore = (1+b**2)*(precision * recall) / (b**2*precision + recall)\n",
    "# locate the index of the largest f score\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n",
    "                                                                        fscore[ix],\n",
    "                                                                        precision[ix],\n",
    "                                                                        recall[ix]))\n",
    "\n",
    "r_auc = roc_auc_score(y_true=y_test, y_score=knn.predict_proba(X_test)[:,1])\n",
    "l_los = log_loss(y_true=y_test, y_pred=knn.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(\"roc auc score: {}\".format(r_auc))\n",
    "print(\"log loss score: {}\".format(l_los))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6185483870967742, 0.322108843537415, 0.677891156462585)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_score>thresholds[ix])\n",
    "\n",
    "TN = cnf_matrix[0][0]\n",
    "FN = cnf_matrix[1][0]\n",
    "TP = cnf_matrix[1][1]\n",
    "FP = cnf_matrix[0][1]\n",
    "\n",
    "TPR = TP/(TP+FN)\n",
    "FPR = FP/(FP+TN)\n",
    "TNR = TN/(FP+TN)\n",
    "TPR, FPR, TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tab.loc['KNeighborsClassifier', :] = [thresholds[ix], fscore[ix], precision[ix], recall[ix], r_auc, l_los, TPR, FPR, TNR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Threshold</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>roc_auc_s</th>\n",
       "      <th>log_loss_s</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.386937</td>\n",
       "      <td>0.730323</td>\n",
       "      <td>0.647431</td>\n",
       "      <td>0.837558</td>\n",
       "      <td>0.784035</td>\n",
       "      <td>0.57796</td>\n",
       "      <td>0.837442</td>\n",
       "      <td>0.448866</td>\n",
       "      <td>0.551134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.685863</td>\n",
       "      <td>0.588468</td>\n",
       "      <td>0.821889</td>\n",
       "      <td>0.694088</td>\n",
       "      <td>2.04956</td>\n",
       "      <td>0.618548</td>\n",
       "      <td>0.322109</td>\n",
       "      <td>0.677891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Best Threshold   F-Score Precision    Recall roc_auc_s  \\\n",
       "LogisticRegression         0.386937  0.730323  0.647431  0.837558  0.784035   \n",
       "KNeighborsClassifier            0.4  0.685863  0.588468  0.821889  0.694088   \n",
       "\n",
       "                     log_loss_s       TPR       FPR       TNR  \n",
       "LogisticRegression      0.57796  0.837442  0.448866  0.551134  \n",
       "KNeighborsClassifier    2.04956  0.618548  0.322109  0.677891  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score is 0.7734501681056019+-0.007171140345435727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "r_forest = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('r_forest', RandomForestClassifier(random_state=42)),\n",
    "])\n",
    "\n",
    "#запустим кросс-валидацию\n",
    "cv_scores = cross_val_score(r_forest, X_train, y_train, cv=16, scoring='roc_auc')\n",
    "cv_score = np.mean(cv_scores)\n",
    "cv_score_std = np.std(cv_scores)\n",
    "print('CV score is {}+-{}'.format(cv_score, cv_score_std))\n",
    "\n",
    "#обучим пайплайн на всем тренировочном датасете\n",
    "r_forest.fit(X_train, y_train)\n",
    "y_score = r_forest.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.350000, F-Score=0.719, Precision=0.643, Recall=0.816\n",
      "roc auc score: 0.7710366181802983\n",
      "log loss score: 0.5992984853728378\n"
     ]
    }
   ],
   "source": [
    "b=1\n",
    "precision, recall, thresholds = precision_recall_curve(y_test.values, y_score)\n",
    "fscore = (1+b**2)*(precision * recall) / (b**2*precision + recall)\n",
    "# locate the index of the largest f score\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n",
    "                                                                        fscore[ix],\n",
    "                                                                        precision[ix],\n",
    "                                                                        recall[ix]))\n",
    "\n",
    "r_auc = roc_auc_score(y_true=y_test, y_score=r_forest.predict_proba(X_test)[:,1])\n",
    "l_los = log_loss(y_true=y_test, y_pred=r_forest.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(\"roc auc score: {}\".format(r_auc))\n",
    "print(\"log loss score: {}\".format(l_los))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8084101382488479, 0.43412698412698414, 0.5658730158730159)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_score>thresholds[ix])\n",
    "\n",
    "TN = cnf_matrix[0][0]\n",
    "FN = cnf_matrix[1][0]\n",
    "TP = cnf_matrix[1][1]\n",
    "FP = cnf_matrix[0][1]\n",
    "\n",
    "TPR = TP/(TP+FN)\n",
    "FPR = FP/(FP+TN)\n",
    "TNR = TN/(FP+TN)\n",
    "TPR, FPR, TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Threshold</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>roc_auc_s</th>\n",
       "      <th>log_loss_s</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.386937</td>\n",
       "      <td>0.730323</td>\n",
       "      <td>0.647431</td>\n",
       "      <td>0.837558</td>\n",
       "      <td>0.784035</td>\n",
       "      <td>0.57796</td>\n",
       "      <td>0.837442</td>\n",
       "      <td>0.448866</td>\n",
       "      <td>0.551134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.685863</td>\n",
       "      <td>0.588468</td>\n",
       "      <td>0.821889</td>\n",
       "      <td>0.694088</td>\n",
       "      <td>2.04956</td>\n",
       "      <td>0.618548</td>\n",
       "      <td>0.322109</td>\n",
       "      <td>0.677891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.718863</td>\n",
       "      <td>0.642669</td>\n",
       "      <td>0.815553</td>\n",
       "      <td>0.771037</td>\n",
       "      <td>0.599298</td>\n",
       "      <td>0.80841</td>\n",
       "      <td>0.434127</td>\n",
       "      <td>0.565873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Best Threshold   F-Score Precision    Recall roc_auc_s  \\\n",
       "LogisticRegression           0.386937  0.730323  0.647431  0.837558  0.784035   \n",
       "KNeighborsClassifier              0.4  0.685863  0.588468  0.821889  0.694088   \n",
       "RandomForestClassifier           0.35  0.718863  0.642669  0.815553  0.771037   \n",
       "\n",
       "                       log_loss_s       TPR       FPR       TNR  \n",
       "LogisticRegression        0.57796  0.837442  0.448866  0.551134  \n",
       "KNeighborsClassifier      2.04956  0.618548  0.322109  0.677891  \n",
       "RandomForestClassifier   0.599298   0.80841  0.434127  0.565873  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tab.loc['RandomForestClassifier', :] = [thresholds[ix], fscore[ix], precision[ix], recall[ix], \n",
    "                                            r_auc, l_los, TPR, FPR, TNR]\n",
    "res_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score is 0.8025125910838183+-0.00707472977074522\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('gbc', GradientBoostingClassifier(random_state=42)),\n",
    "])\n",
    "\n",
    "#запустим кросс-валидацию\n",
    "cv_scores = cross_val_score(gbc, X_train, y_train, cv=16, scoring='roc_auc')\n",
    "cv_score = np.mean(cv_scores)\n",
    "cv_score_std = np.std(cv_scores)\n",
    "print('CV score is {}+-{}'.format(cv_score, cv_score_std))\n",
    "\n",
    "#обучим пайплайн на всем тренировочном датасете\n",
    "gbc.fit(X_train, y_train)\n",
    "y_score = gbc.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.394947, F-Score=0.740, Precision=0.698, Recall=0.788\n",
      "roc auc score: 0.8026153641179974\n",
      "log loss score: 0.5397460438742135\n"
     ]
    }
   ],
   "source": [
    "b=1\n",
    "precision, recall, thresholds = precision_recall_curve(y_test.values, y_score)\n",
    "fscore = (1+b**2)*(precision * recall) / (b**2*precision + recall)\n",
    "# locate the index of the largest f score\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n",
    "                                                                        fscore[ix],\n",
    "                                                                        precision[ix],\n",
    "                                                                        recall[ix]))\n",
    "\n",
    "r_auc = roc_auc_score(y_true=y_test, y_score=gbc.predict_proba(X_test)[:,1])\n",
    "l_los = log_loss(y_true=y_test, y_pred=gbc.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(\"roc auc score: {}\".format(r_auc))\n",
    "print(\"log loss score: {}\".format(l_los))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7880184331797235, 0.33582766439909295, 0.664172335600907)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_score>thresholds[ix])\n",
    "\n",
    "TN = cnf_matrix[0][0]\n",
    "FN = cnf_matrix[1][0]\n",
    "TP = cnf_matrix[1][1]\n",
    "FP = cnf_matrix[0][1]\n",
    "\n",
    "TPR = TP/(TP+FN)\n",
    "FPR = FP/(FP+TN)\n",
    "TNR = TN/(FP+TN)\n",
    "TPR, FPR, TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Threshold</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>roc_auc_s</th>\n",
       "      <th>log_loss_s</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.386937</td>\n",
       "      <td>0.730323</td>\n",
       "      <td>0.647431</td>\n",
       "      <td>0.837558</td>\n",
       "      <td>0.784035</td>\n",
       "      <td>0.57796</td>\n",
       "      <td>0.837442</td>\n",
       "      <td>0.448866</td>\n",
       "      <td>0.551134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.685863</td>\n",
       "      <td>0.588468</td>\n",
       "      <td>0.821889</td>\n",
       "      <td>0.694088</td>\n",
       "      <td>2.04956</td>\n",
       "      <td>0.618548</td>\n",
       "      <td>0.322109</td>\n",
       "      <td>0.677891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.718863</td>\n",
       "      <td>0.642669</td>\n",
       "      <td>0.815553</td>\n",
       "      <td>0.771037</td>\n",
       "      <td>0.599298</td>\n",
       "      <td>0.80841</td>\n",
       "      <td>0.434127</td>\n",
       "      <td>0.565873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingClassifier</th>\n",
       "      <td>0.394947</td>\n",
       "      <td>0.740248</td>\n",
       "      <td>0.697848</td>\n",
       "      <td>0.788134</td>\n",
       "      <td>0.802615</td>\n",
       "      <td>0.539746</td>\n",
       "      <td>0.788018</td>\n",
       "      <td>0.335828</td>\n",
       "      <td>0.664172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Best Threshold   F-Score Precision    Recall  \\\n",
       "LogisticRegression               0.386937  0.730323  0.647431  0.837558   \n",
       "KNeighborsClassifier                  0.4  0.685863  0.588468  0.821889   \n",
       "RandomForestClassifier               0.35  0.718863  0.642669  0.815553   \n",
       "GradientBoostingClassifier       0.394947  0.740248  0.697848  0.788134   \n",
       "\n",
       "                           roc_auc_s log_loss_s       TPR       FPR       TNR  \n",
       "LogisticRegression          0.784035    0.57796  0.837442  0.448866  0.551134  \n",
       "KNeighborsClassifier        0.694088    2.04956  0.618548  0.322109  0.677891  \n",
       "RandomForestClassifier      0.771037   0.599298   0.80841  0.434127  0.565873  \n",
       "GradientBoostingClassifier  0.802615   0.539746  0.788018  0.335828  0.664172  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tab.loc['GradientBoostingClassifier', :] = [thresholds[ix], fscore[ix], precision[ix], recall[ix], \n",
    "                                                r_auc, l_los, TPR, FPR, TNR]\n",
    "res_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\viv232\\anaconda3\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\viv232\\anaconda3\\lib\\site-packages (from xgboost) (1.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\viv232\\anaconda3\\lib\\site-packages (from xgboost) (1.18.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:29:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:29:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:29:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:29:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:29:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:29:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:29:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:29:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:30:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:30:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:30:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:30:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:30:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:30:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:30:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:30:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:30:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:31:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:31:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:31:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:31:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:31:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:31:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:31:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[08:31:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CV score is 0.7974879407860951+-0.006447877492510205\n",
      "[08:31:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viv232\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:31:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgbc = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('xgbc', XGBClassifier(objective='binary:logistic', silent=True, random_state=42)),\n",
    "])\n",
    "\n",
    "#запустим кросс-валидацию\n",
    "cv_scores = cross_val_score(xgbc, X_train, y_train, cv=16, scoring='roc_auc')\n",
    "cv_score = np.mean(cv_scores)\n",
    "cv_score_std = np.std(cv_scores)\n",
    "print('CV score is {}+-{}'.format(cv_score, cv_score_std))\n",
    "\n",
    "#обучим пайплайн на всем тренировочном датасете\n",
    "xgbc.fit(X_train, y_train)\n",
    "y_score = xgbc.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.347103, F-Score=0.738, Precision=0.665, Recall=0.828\n",
      "roc auc score: 0.797227760535858\n",
      "log loss score: 0.5471106075466078\n"
     ]
    }
   ],
   "source": [
    "b=1\n",
    "precision, recall, thresholds = precision_recall_curve(y_test.values, y_score)\n",
    "fscore = (1+b**2)*(precision * recall) / (b**2*precision + recall)\n",
    "# locate the index of the largest f score\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n",
    "                                                                        fscore[ix],\n",
    "                                                                        precision[ix],\n",
    "                                                                        recall[ix]))\n",
    "\n",
    "r_auc = roc_auc_score(y_true=y_test, y_score=xgbc.predict_proba(X_test)[:,1])\n",
    "l_los = log_loss(y_true=y_test, y_pred=xgbc.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(\"roc auc score: {}\".format(r_auc))\n",
    "print(\"log loss score: {}\".format(l_los))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8282258064516129, 0.41020408163265304, 0.5897959183673469)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_score>thresholds[ix])\n",
    "\n",
    "TN = cnf_matrix[0][0]\n",
    "FN = cnf_matrix[1][0]\n",
    "TP = cnf_matrix[1][1]\n",
    "FP = cnf_matrix[0][1]\n",
    "\n",
    "TPR = TP/(TP+FN)\n",
    "FPR = FP/(FP+TN)\n",
    "TNR = TN/(FP+TN)\n",
    "TPR, FPR, TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Threshold</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>roc_auc_s</th>\n",
       "      <th>log_loss_s</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.386937</td>\n",
       "      <td>0.730323</td>\n",
       "      <td>0.647431</td>\n",
       "      <td>0.837558</td>\n",
       "      <td>0.784035</td>\n",
       "      <td>0.57796</td>\n",
       "      <td>0.837442</td>\n",
       "      <td>0.448866</td>\n",
       "      <td>0.551134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.685863</td>\n",
       "      <td>0.588468</td>\n",
       "      <td>0.821889</td>\n",
       "      <td>0.694088</td>\n",
       "      <td>2.04956</td>\n",
       "      <td>0.618548</td>\n",
       "      <td>0.322109</td>\n",
       "      <td>0.677891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.718863</td>\n",
       "      <td>0.642669</td>\n",
       "      <td>0.815553</td>\n",
       "      <td>0.771037</td>\n",
       "      <td>0.599298</td>\n",
       "      <td>0.80841</td>\n",
       "      <td>0.434127</td>\n",
       "      <td>0.565873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingClassifier</th>\n",
       "      <td>0.394947</td>\n",
       "      <td>0.740248</td>\n",
       "      <td>0.697848</td>\n",
       "      <td>0.788134</td>\n",
       "      <td>0.802615</td>\n",
       "      <td>0.539746</td>\n",
       "      <td>0.788018</td>\n",
       "      <td>0.335828</td>\n",
       "      <td>0.664172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.347103</td>\n",
       "      <td>0.73789</td>\n",
       "      <td>0.665248</td>\n",
       "      <td>0.828341</td>\n",
       "      <td>0.797228</td>\n",
       "      <td>0.547111</td>\n",
       "      <td>0.828226</td>\n",
       "      <td>0.410204</td>\n",
       "      <td>0.589796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Best Threshold   F-Score Precision    Recall  \\\n",
       "LogisticRegression               0.386937  0.730323  0.647431  0.837558   \n",
       "KNeighborsClassifier                  0.4  0.685863  0.588468  0.821889   \n",
       "RandomForestClassifier               0.35  0.718863  0.642669  0.815553   \n",
       "GradientBoostingClassifier       0.394947  0.740248  0.697848  0.788134   \n",
       "XGBClassifier                    0.347103   0.73789  0.665248  0.828341   \n",
       "\n",
       "                           roc_auc_s log_loss_s       TPR       FPR       TNR  \n",
       "LogisticRegression          0.784035    0.57796  0.837442  0.448866  0.551134  \n",
       "KNeighborsClassifier        0.694088    2.04956  0.618548  0.322109  0.677891  \n",
       "RandomForestClassifier      0.771037   0.599298   0.80841  0.434127  0.565873  \n",
       "GradientBoostingClassifier  0.802615   0.539746  0.788018  0.335828  0.664172  \n",
       "XGBClassifier               0.797228   0.547111  0.828226  0.410204  0.589796  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tab.loc['XGBClassifier', :] = [thresholds[ix], fscore[ix], precision[ix], recall[ix], \n",
    "                                                r_auc, l_los, TPR, FPR, TNR]\n",
    "res_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итоговая таблица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Threshold</th>\n",
       "      <th>F-Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>roc_auc_s</th>\n",
       "      <th>log_loss_s</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GradientBoostingClassifier</th>\n",
       "      <td>0.394947</td>\n",
       "      <td>0.740248</td>\n",
       "      <td>0.697848</td>\n",
       "      <td>0.788134</td>\n",
       "      <td>0.802615</td>\n",
       "      <td>0.539746</td>\n",
       "      <td>0.788018</td>\n",
       "      <td>0.335828</td>\n",
       "      <td>0.664172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.347103</td>\n",
       "      <td>0.73789</td>\n",
       "      <td>0.665248</td>\n",
       "      <td>0.828341</td>\n",
       "      <td>0.797228</td>\n",
       "      <td>0.547111</td>\n",
       "      <td>0.828226</td>\n",
       "      <td>0.410204</td>\n",
       "      <td>0.589796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.386937</td>\n",
       "      <td>0.730323</td>\n",
       "      <td>0.647431</td>\n",
       "      <td>0.837558</td>\n",
       "      <td>0.784035</td>\n",
       "      <td>0.57796</td>\n",
       "      <td>0.837442</td>\n",
       "      <td>0.448866</td>\n",
       "      <td>0.551134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.718863</td>\n",
       "      <td>0.642669</td>\n",
       "      <td>0.815553</td>\n",
       "      <td>0.771037</td>\n",
       "      <td>0.599298</td>\n",
       "      <td>0.80841</td>\n",
       "      <td>0.434127</td>\n",
       "      <td>0.565873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.685863</td>\n",
       "      <td>0.588468</td>\n",
       "      <td>0.821889</td>\n",
       "      <td>0.694088</td>\n",
       "      <td>2.04956</td>\n",
       "      <td>0.618548</td>\n",
       "      <td>0.322109</td>\n",
       "      <td>0.677891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Best Threshold   F-Score Precision    Recall  \\\n",
       "GradientBoostingClassifier       0.394947  0.740248  0.697848  0.788134   \n",
       "XGBClassifier                    0.347103   0.73789  0.665248  0.828341   \n",
       "LogisticRegression               0.386937  0.730323  0.647431  0.837558   \n",
       "RandomForestClassifier               0.35  0.718863  0.642669  0.815553   \n",
       "KNeighborsClassifier                  0.4  0.685863  0.588468  0.821889   \n",
       "\n",
       "                           roc_auc_s log_loss_s       TPR       FPR       TNR  \n",
       "GradientBoostingClassifier  0.802615   0.539746  0.788018  0.335828  0.664172  \n",
       "XGBClassifier               0.797228   0.547111  0.828226  0.410204  0.589796  \n",
       "LogisticRegression          0.784035    0.57796  0.837442  0.448866  0.551134  \n",
       "RandomForestClassifier      0.771037   0.599298   0.80841  0.434127  0.565873  \n",
       "KNeighborsClassifier        0.694088    2.04956  0.618548  0.322109  0.677891  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tab.sort_values('roc_auc_s', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С гиперпараметрами по умолчанию лучший результат показал GradientBoostingClassifier. Однако для каждого метода желательно произвести подбор гиперпараметов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
